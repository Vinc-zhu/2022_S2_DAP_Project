---
title: "DAP_Project"
author: "Jinfeng Zhu"
date: "2022-08-15"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Installing libraries
# install.packages("naniar")
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggthemes)

library(janitor) # to clean column names 
library(psych)   # for the describe function

library("stringr") # to add leading zeros
library(naniar) # to check missing values
```

# 1. Exploratory Data Analysis

## 1.1 Import Data

```{r import data, include=FALSE}
data_death      <- read_csv("Data/death.csv")
data_health_ins <- read_csv("Data/healthinsurance.csv")
data_incidence  <- read_csv("Data/incidence.csv")
data_income     <- read_csv("Data/income.csv")
data_population <- read_csv("Data/population.csv")
data_poverty    <- read_csv("Data/poverty.csv")
```

## 1.2 Clean variable names

```{r naming convention of header}
data_death_clean      <- clean_names(data_death)
data_health_ins_clean <- clean_names(data_health_ins)
data_incidence_clean  <- clean_names(data_incidence)
data_income_clean     <- clean_names(data_income)
data_population_clean <- clean_names(data_population)
data_poverty_clean    <- clean_names(data_poverty)

names(data_death_clean)
names(data_health_ins_clean)
names(data_incidence_clean)
names(data_income_clean)       
names(data_population_clean)  
names(data_poverty_clean)

```

## 1.3 Data Exploration

```{r view data}
# Overall checks
head(data_death_clean) # "age_adjusted_death_rate" and “average_deaths_per_year” are character
head(data_health_ins_clean) 
head(data_incidence_clean) # "age_adjusted_incidence_rate_cases_per_100_000" and “average_annual_count” are character
head(data_income_clean)
head(data_population_clean)
head(data_poverty_clean)

summary(data_death_clean) 
summary(data_health_ins_clean) 
summary(data_incidence_clean)
summary(data_income_clean)
summary(data_population_clean)
summary(data_poverty_clean)
```

### 1.3.1 Check duplicates

```{r check duplicates}
# Check duplicates
data_death_clean %>% mutate(dup = duplicated(data_death_clean)) %>% filter(dup == TRUE)
data_health_ins_clean %>% mutate(dup = duplicated(data_health_ins_clean)) %>% filter(dup == TRUE)
data_incidence_clean %>% mutate(dup = duplicated(data_incidence_clean)) %>% filter(dup == TRUE)
data_income_clean %>% mutate(dup = duplicated(data_income_clean)) %>% filter(dup == TRUE)
data_population_clean %>% mutate(dup = duplicated(data_population_clean)) %>% filter(dup == TRUE)
data_poverty_clean %>% mutate(dup = duplicated(data_poverty_clean)) %>% filter(dup == TRUE)
  # No duplicates for all datasets
```

### 1.3.2 Convert Data Type

```{r data type conversion}
# [Death] Convert the data type from character to numeric
data_death_clean2 <- data_death_clean %>%
  mutate(age_adjusted_death_rate=as.double(age_adjusted_death_rate)) %>%
  mutate(average_deaths_per_year=as.double(average_deaths_per_year))

miss_var_summary(data_death_clean)
miss_var_summary(data_death_clean2)
  # 331 suppressed values "*" are automatically converted to NA by coercion


# [Incidence] Convert the data type from character to numeric
data_incidence_clean2 <- data_incidence_clean %>%
  mutate(age_adjusted_incidence_rate_cases_per_100_000=as.double(age_adjusted_incidence_rate_cases_per_100_000)) %>%
  mutate(average_annual_count=as.double(average_annual_count))

miss_var_summary(data_incidence_clean)
miss_var_summary(data_incidence_clean2)
  # 442 suppressed values "*" are automatically converted to NA by coercion

# [Income] Convert the data type from character to numeric
summary(data_income_clean)

data_income_clean$income_b_001 <- as.double(data_income_clean$income_b_001)
data_income_clean$income_c_001 <- as.double(data_income_clean$income_c_001)
data_income_clean$income_d_001 <- as.double(data_income_clean$income_d_001)
data_income_clean$income_e_001 <- as.double(data_income_clean$income_e_001)
data_income_clean$income_f_001 <- as.double(data_income_clean$income_f_001)
data_income_clean$income_g_001 <- as.double(data_income_clean$income_g_001)
data_income_clean$income_h_001 <- as.double(data_income_clean$income_h_001)
data_income_clean$income_i_001 <- as.double(data_income_clean$income_i_001)

data_income_clean

```

### 1.3.3 Reshaping Data into Tidy Form 

```{r reshaping}
# Separate state names from county
data_death_state <- data_death_clean2 %>% separate(county, c("county_name","state_name"), ", ")
  # Missing pieces are autimatically filled with `NA` in 2 rows, needs to check 

data_death_state %>% filter(is.na(county_name)) # no "NA"
data_death_state %>% filter(is.na(state_name))  # one is United States, the other is District of Columbia (State) (DC)

data_death_state1 <- data_death_state %>% mutate(state_name  = ifelse(county_name == "District of Columbia (State)", "DC", state_name))

data_death_state2 <- data_death_state1 %>% mutate(state_name  = ifelse(county_name == "United States", "US", state_name))

data_death_state2 %>% filter(is.na(state_name)) # no more missing values in state_name


# Check the number of the distinct states
n_distinct(data_death_state2$state_name) # Unique number of states is 54. Based on the domain knowledge, the United States is made up of a total of 50 states, plus the District of Columbia.

# Check the list of states
unique(data_death_state2$state_name) # "Arizona<sup>3</sup>" and "Alaska<sup>3</sup>" appear to be errors

data_death_state3 <- data_death_state2 %>% mutate(state_name = ifelse(state_name == "Alaska<sup>3</sup>", "Alaska", state_name))

data_death_state4 <- data_death_state3 %>% mutate(state_name = ifelse(state_name == "Arizona<sup>3</sup>", "Arizona", state_name))

# Check the number of the distinct states again
n_distinct(data_death_state4$state_name) # 52 = 50 states + 1 federal district + US as a whole
unique(data_death_state4$state_name) # looks alright

```

### 1.3.4 Internal Consistency Checks

```{r health insurance}
# Check if Non-institutionalized Population (hi_001) = Non-institutionalized Population_male (hi_002) + Non-institutionalized Population female (hi_030)
data_health_ins_clean %>% 
  mutate(check = hi_001 - hi_002 - hi_030) %>% 
  group_by(state) %>% 
  summarise(sum(check)) 
  # all zero, passed the check
```


```{r poverty}
# Check if:
# 1) below poverty level population (poverty_002) = below poverty level male (poverty_003) + below poverty level female (poverty_017)
data_poverty_clean %>% 
  mutate(check = poverty_002 - poverty_003 - poverty_017) %>% 
  group_by(state) %>% 
  summarise(sum(check))
  # all zero, passed the check

# 2) above poverty level population (poverty_031) = above poverty level male (poverty_032) + above poverty level female (poverty_046)
data_poverty_clean %>% 
  mutate(check = poverty_031 - poverty_032 - poverty_046) %>% 
  group_by(state) %>% 
  summarise(sum(check))
  # all zero, passed the check

# 3) Population For Whom Poverty Status Is Determined (poverty_001) = below poverty level population (poverty_002) + above poverty level population (poverty_031)
data_poverty_clean %>% 
  mutate(check = poverty_001 - poverty_002 - poverty_031) %>% 
  group_by(state) %>% 
  summarise(sum(check))
  # all zero, passed the check

```

```{r population}
# Check the total population at mid-year 2015 on a national level.
sum(data_population_clean$popestimate2015)/10**6 # 642 million

# [Uncertainty] Based on the domain knowledge, the population of the United States should be around 320 million. It seems the number in the population dataset was doubled.

# This can be verified by the death dataset
data_death_state4 %>% 
  mutate(population = average_deaths_per_year / (age_adjusted_death_rate/100000)) %>% 
  filter(state_name == "US") %>% 
  pull(population)/10**6
  # 329 million

# Reduce the entire population data by half to make it more consistent with the real world.
data_population_clean_half <-  data_population_clean %>% mutate(popestimate2015 = popestimate2015/2)

sum(data_population_clean_half$popestimate2015)/10**6 # 321 million

```




















#=======================================================#
to-do:
1.Manipulate and cleanse
i. Clean FIPs
ii. Check dup FIP
iii. Expert opinion

2.Create State_abbr for death dataset

3.Explore ggplot

4.Join the data
#=======================================================#


## 1.3 Assess Data Quality

### 1.3.1 Summarise Data


### 1.3.2 gg plot
```{r}
data_death_clean %>% ggplot(aes(state_name, fill = state_name)) +
  geom_bar(show.legend = T)
#   theme(axis.title.x=element_blank(),
#         axis.text.x=element_blank(),
#         axis.ticks.x=element_blank())

prop.table(table(data_death_clean$state_name))
```

### 1.3.3 Visualise and Analyse Patterns in Data
```{r}
# See where the average of the nation (US) sits among other states
data_death_clean_state <- data_death_clean %>% 
  group_by(state_name) %>% 
  summarize(death_rate = mean(age_adjusted_death_rate, na.rm=TRUE))
data_death_clean_state 

data_death_clean_stat_US <- data_death_clean %>% 
  filter(state_name == "US")
data_death_clean_stat_US

ggplot() +
  geom_col(data = data_death_clean_state, aes(state_name, death_rate)) +
  geom_col(data = data_death_clean_stat_US, aes(state_name, age_adjusted_death_rate, fill = state_name)) +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

# test
ggplot(data_death_clean, aes(average_deaths_per_year, age_adjusted_death_rate, group = state_name)) +
geom_line() +
facet_wrap(. ~ state_name)

# state_death <- data_death_clean_state %>% ggplot(aes(state_name, death_rate, fill = state_name)) +
#   geom_col(show.legend = F) + 
#   theme(axis.title.x=element_blank(),
#         axis.text.x=element_blank(),
#         axis.ticks.x=element_blank())
# 
# ggsave("state_death.jpg", state_death)


```





## 1.4 Manipulate and Cleanse the Data
- The process of cleansing and manipulating the data often comes after you have understood the data and tried to look at patterns in the data through visualization.

### 1.4.1 Clean FIPS

```{r FIPS}
# Make FIPS a 5-digit code 
# For death and incidence data, most of the FIPS codes are 5-digits but if they are 4-digits, you will need to add a 0 in front. 
data_death_clean_fips <- data_death_clean %>% 
  mutate(fips_clean = str_pad(data_death_clean$fips, width = 5, pad = "0"))

data_incidence_clean_fips <- data_incidence_clean %>% 
  mutate(fips_clean = str_pad(data_incidence_clean$fips, width = 5, pad = "0"))

# For poverty, health insurance, income and population data, you will need to combine State FIPS and County FIPS to get the 5-digit code.
data_health_ins_clean_fips <- data_health_ins_clean %>% 
  mutate(state_fips_clean  = str_pad(data_health_ins_clean$state_fips, width = 2, pad = "0")) %>% 
  mutate(county_fips_clean = str_pad(data_health_ins_clean$county_fips, width = 3, pad = "0")) %>% 
  mutate(fips_clean = paste(state_fips_clean, county_fips_clean, sep=""))

data_income_clean_fips <- data_income_clean %>% 
  mutate(state_fips_clean  = str_pad(data_income_clean$state_fips, width = 2, pad = "0")) %>% 
  mutate(county_fips_clean = str_pad(data_income_clean$county_fips, width = 3, pad = "0")) %>% 
  mutate(fips_clean = paste(state_fips_clean, county_fips_clean, sep=""))

data_population_clean_fips <- data_population_clean %>% 
  mutate(state_fips_clean  = str_pad(data_population_clean$state, width = 2, pad = "0")) %>% 
  mutate(county_fips_clean = str_pad(data_population_clean$county, width = 3, pad = "0")) %>% 
  mutate(fips_clean = paste(state_fips_clean, county_fips_clean, sep=""))

data_poverty_clean_fips <- data_poverty_clean %>% 
  mutate(state_fips_clean  = str_pad(data_poverty_clean$state_fips, width = 2, pad = "0")) %>% 
  mutate(county_fips_clean = str_pad(data_poverty_clean$county_fips, width = 3, pad = "0")) %>% 
  mutate(fips_clean = paste(state_fips_clean, county_fips_clean, sep=""))


# Check in EXCEL: 
# write_excel_csv(data_population_clean_fips, "population_clean_fips.csv", col_names = TRUE)

```

### 1.4.2 Check Duplicate FIPS
distinct() shows you which rows of the dataframe are distinct i.e. removes the duplicate rows.
To find out the duplicates, you can use the function duplicated().

```{r check FIPS}
# data_death_clean_fips %>% summarise(n_distinct(fips_clean))

duplicated(gapminder_allwithkey$key) #returns logical vector
gapminder_allwithkey$key[duplicated(gapminder_allwithkey$key)] #to check any duplicates in key

dim(data_death_clean_fips)[1] == length(unique(data_death_clean_fips$fips_clean)) # No duplicates
dim(data_incidence_clean_fips)[1] == length(unique(data_incidence_clean_fips$fips_clean)) # No duplicates
dim(data_health_ins_clean_fips)[1] == length(unique(data_health_ins_clean_fips$fips_clean)) # No duplicates
dim(data_income_clean_fips)[1] == length(unique(data_income_clean_fips$fips_clean)) # No duplicates
dim(data_population_clean_fips)[1] == length(unique(data_population_clean_fips$fips_clean)) # No duplicates
dim(data_poverty_clean_fips)[1] == length(unique(data_poverty_clean_fips$fips_clean)) # No duplicates 
```

### 1.4.3 Expert Opinion

```{r}
# For Incidence dataset, expert opinion suggests suppressed cells of * for Recent Trend is likely to be stable.
# table(data_incidence_clean_fips$recent_trend)

data_incidence_clean_fips_expert <- data_incidence_clean_fips %>% mutate(recent_trend, recent_trend = ifelse(age_adjusted_incidence_rate_cases_per_100_000 == "*", "stable", recent_trend))

table(data_incidence_clean_fips_expert$recent_trend)


# Apply Dummy Encoding for categorical variable "recent_trend"
data_incidence_clean_fips_expert_encoding <- data_incidence_clean_fips_expert %>%
  mutate(trend_falling = ifelse(recent_trend == "falling", 1, 0)) %>%
  mutate(trend_rising  = ifelse(recent_trend == "rising", 1, 0))

table(data_incidence_clean_fips_expert_encoding$trend_falling)
table(data_incidence_clean_fips_expert_encoding$trend_rising)

```
### 1.4.4 Imbalanced Data (maybe for data_all)



## 1.5 Join Datasets

```{r include=FALSE}
# Step 1
dim(data_death_clean_fips)
dim(data_incidence_clean_fips_expert_encoding)
names(data_incidence_clean_fips_expert_encoding)

# Remove columns that are already contained in the left-hand data
data_incidence_clean_fips_for_join <- data_incidence_clean_fips_expert_encoding %>% 
  select(c(3:6))

# Left Join - death / incidence
data1 <- left_join(data_death_clean_fips, data_incidence_clean_fips_for_join, by="fips_clean")
names(data1)

# Step 2
dim(data_health_ins_clean_fips)
names(data_health_ins_clean_fips)

# Remove columns that are already contained in the left-hand data
data_health_ins_clean_fips_for_join <- data_health_ins_clean_fips %>% 
  select(c(1, 5:61,64))

# Left Join - death / incidence / health
data2 <- left_join(data1, data_health_ins_clean_fips_for_join, by="fips_clean")
names(data2)

# Step 3
dim(data_income_clean_fips)
names(data_income_clean_fips)

# Remove columns that are already contained in the left-hand data
data_income_clean_fips_for_join <- data_income_clean_fips %>% 
  select(c(5:14,17))

# Left Join - death / incidence / health / income
data3 <- left_join(data2, data_income_clean_fips_for_join, by="fips_clean")
names(data3)

# Step 4
dim(data_population_clean_fips)
names(data_population_clean_fips)

# Remove columns that are already contained in the left-hand data
data_population_clean_fips_for_join <- data_population_clean_fips %>% 
  select(c(5,8))

# Left Join - death / incidence / health / income / population
data4 <- left_join(data3, data_population_clean_fips_for_join, by="fips_clean")
names(data4)

# Step 5
dim(data_poverty_clean_fips)
names(data_poverty_clean_fips)

# Remove columns that are already contained in the left-hand data
data_poverty_clean_fips_for_join <- data_poverty_clean_fips %>% 
  select(c(5:63,66))

# Left Join - death / incidence / health / income / population / poverty
data_all <- left_join(data4, data_poverty_clean_fips_for_join, by="fips_clean")

glimpse(data_all)

# Check in EXCEL: 
# write_excel_csv(data_all, "data_all.csv", col_names = TRUE)

```

## 1.6 Check Missing Values

-   are those that have been suppressed to ensure confidentiality
missing at random: omit it
missing for a specific group: impute

gapminder_all %>% filter(!is.na(population)) %>%
summarise(mean_pop = mean(population), sd_pop = sd(population))

```{r}
# Replace all suppressed value "*" to "NA"
data_all_with_NA <- data_all %>% mutate(across(everything(), function(x){ifelse(x == "*" | x == ".", NA, x)}))
glimpse(data_all_with_NA)
  
miss_var_summary(data_all)
miss_var_summary(data_all_with_NA)
```

## 1.7 Visualising
- Communicate a data driven finding
- Useful if difficult to extract insights from tables of raw values


See Week 4 Notes Page 7

```{r}

# theme_economist()
# theme_fivethirtyeight()
# theme_bw()

# Take a glance at stroke mortality rate by state
summary_data_all_state <- data_all %>% group_by(state) %>% summarize(death_rate = mean(age_adjusted_death_rate, na.rm=TRUE))
summary_data_all_state 

summary_data_all_state %>% ggplot(aes(state, death_rate)) +
  geom_col() + theme_economist()
  


library("corrplot") 
# for numeric variables [TBD]
corrplot(cor(data_all[, 13:44], use="pairwise.complete.obs"))




# The advantage of faceting compared to grid.arrange() below is that it keeps the axes fixed across all plots, easing comparisons between plots.


#==========Example=========#
# exposuretotal <- summary_veh_brand %>% select(veh_brand, exposuretot)
# summary_veh_brand_area <- motor_data %>% group_by(veh_brand, area) %>% summarise(exposure=sum(exposure), claim_num = sum(claim_nb))
# 
# summary_veh_brand_area_total <- left_join(summary_veh_brand_area,summary_veh_brand, by="veh_brand") %>%
#   mutate(incr_freq = claim_num/exposuretot, rel_freq = incr_freq/frequency)
# 
# summary_veh_brand_area_total
# #write_excel_csv(summary_veh_brand_area_total, "Test_Vinc.csv", col_names = TRUE)
# 
# summary_veh_brand_area_total %>% ggplot(aes(veh_brand, rel_freq, fill=area)) +
#   geom_bar(stat = 'identity') #stat identity allows you to make to a y value for bar
#==========================#


# data_all %>% ggplot(aes(state, age_adjusted_death_rate), ) + 
#   geom_boxplot()
#   theme(legend.position="none" #不需要图例
#         # axis.text.x=element_text(colour="black",family="Times",size=14), #设置x轴刻度标签的字体属性
#         # axis.text.y=element_text(family="Times",size=14,face="plain"), #设置x轴刻度标签的字体属性
#         # axis.title.y=element_text(family="Times",size = 14,face="plain"), #设置y轴的标题的字体属性
#         # axis.title.x=element_text(family="Times",size = 14,face="plain"), #设置x轴的标题的字体属性
#         # plot.title = element_text(family="Times",size=15,face="bold",hjust = 0.5), #设置总标题的字体属性
#         # panel.grid.major = element_blank(), #不显示网格线
#         # panel.grid.minor = element_blank()
#         )
#   + scale_y_continuous(breaks = NULL)

```

## 1.X Refine questions/generate new questions (optional)

EDA should not be viewed as just conducting the above framework as a
series of ordered steps. This is far too simplistic as EDA typically
involves multiple rounds of each step. For example, visualisations could
give you new ideas for summaries you would be interested in and perhaps
patterns you want to analyse, which could lead to more manipulations to
the data and visualisations.

### Internal Checks

# Correlation

```{r}
describe(data_poverty)
```

Exploratory data analysis

Objectives: •Suggest hypotheses •Assess assumptions made •Support
selection of appropriate models •Provide basis for further data
collection

FrameworK: 1. Assess the data quality

# 提取各个模型的诊断信息

by_country %\>% summarise(glance(model))
